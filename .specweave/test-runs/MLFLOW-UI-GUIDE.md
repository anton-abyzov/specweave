# MLflow UI - Visual Guide

## âœ… MLflow is Running!

**URL**: http://localhost:5000

**Status**: âœ… Server is running with 4 workers

---

## What You'll See

### 1. Main Experiments Page

When you open http://localhost:5000, you'll see:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MLflow Tracking                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Experiments                                                â”‚
â”‚  â”œâ”€â”€ Default (0 runs)                                       â”‚
â”‚  â””â”€â”€ soccer-player-detection (4 runs) â­                    â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Click on "soccer-player-detection"** to see your experiments!

---

### 2. Experiment Runs Table

You'll see a table with 4 rows (our 4 training runs):

| Run Name | Created | Duration | accuracy | precision | recall | f1_score | training_time |
|----------|---------|----------|----------|-----------|--------|----------|---------------|
| **medium_model_balanced** ðŸ† | Just now | 0.028s | 1.000 | 1.000 | 1.000 | 1.000 | 0.028s |
| large_model_accurate | Just now | 0.056s | 1.000 | 1.000 | 1.000 | 1.000 | 0.056s |
| small_model_fast | Just now | 0.007s | 1.000 | 1.000 | 1.000 | 1.000 | 0.007s |
| overfitting_model_deep âš ï¸ | Just now | 0.028s | 1.000 | 1.000 | 1.000 | 1.000 | 0.028s |

**Key Features**:
- **ðŸ† Production Candidate**: The medium model is tagged for production
- **âš ï¸ Experimental**: The overfitting model has a warning tag
- **Sortable**: Click column headers to sort by any metric
- **Searchable**: Filter runs by name or tags

---

### 3. Click on a Run to See Details

When you click on **"medium_model_balanced"**, you'll see:

#### Parameters Tab
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Parameters                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ model_type: RandomForest        â”‚
â”‚ n_estimators: 50                â”‚
â”‚ max_depth: 10                   â”‚
â”‚ dataset_size: 800               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Metrics Tab
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metrics                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ accuracy: 1.000                 â”‚
â”‚ precision: 1.000                â”‚
â”‚ recall: 1.000                   â”‚
â”‚ f1_score: 1.000                 â”‚
â”‚ training_time_seconds: 0.028    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Tags Tab
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tags                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ stage: production_candidate     â”‚
â”‚ owner: ml_team                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Artifacts Tab (Trained Model!)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Artifacts                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ðŸ“ player_detector_medium/      â”‚
â”‚   â”œâ”€â”€ MLmodel                   â”‚
â”‚   â”œâ”€â”€ conda.yaml                â”‚
â”‚   â”œâ”€â”€ model.pkl                 â”‚ â† Your trained model!
â”‚   â”œâ”€â”€ python_env.yaml           â”‚
â”‚   â””â”€â”€ requirements.txt          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**You can download `model.pkl` and use it in production!**

---

### 4. Compare Runs

Click **"Compare"** button at the top, then select multiple runs:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Parallel Coordinates Plot                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚   Parameters          Metrics                                â”‚
â”‚   n_estimators â”€â”€â”€â”€â”€â”€â”€> accuracy                            â”‚
â”‚      10 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> 1.000                              â”‚
â”‚      50 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> 1.000                              â”‚
â”‚     100 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> 1.000                              â”‚
â”‚                                                               â”‚
â”‚   max_depth â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> training_time                       â”‚
â”‚       5 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> 0.007s                             â”‚
â”‚      10 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> 0.028s                             â”‚
â”‚      20 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> 0.056s                             â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Visual insights**:
- See how parameters affect metrics
- Identify trade-offs (speed vs accuracy)
- Find optimal configurations

---

### 5. Search and Filter

In the search box, try:

**Filter by tags**:
```
tags.stage = "production_candidate"
```
Result: Shows only the medium model

**Filter by metrics**:
```
metrics.training_time_seconds < 0.03
```
Result: Shows small and medium models (fast ones)

**Filter by parameters**:
```
params.n_estimators >= 50
```
Result: Shows medium and large models

---

## What This Demonstrates

### 1. Experiment Tracking in Action

Instead of manually tracking results in a spreadsheet:

**Before MLflow**:
```
# notes.txt
Run 1: n_estimators=10, accuracy=???  # forgot to save!
Run 2: n_estimators=50, accuracy=1.0  # but which model file?
Run 3: n_estimators=100 # where did I save this?
```

**With MLflow**:
- Everything automatically tracked
- Never lose an experiment
- Always know which model is which

---

### 2. Model Versioning

Notice how each run has its own saved model:
- `player_detector_small/model.pkl`
- `player_detector_medium/model.pkl`
- `player_detector_large/model.pkl`
- `player_detector_overfit/model.pkl`

**You can load ANY of these models later**:
```python
import mlflow.sklearn

# Load the production candidate model
model = mlflow.sklearn.load_model("runs:/<RUN_ID>/player_detector_medium")

# Use for predictions
predictions = model.predict(new_data)
```

---

### 3. Collaboration

If your team member asks:
> "Which model should I deploy?"

**Answer**:
1. Open http://localhost:5000
2. Look for runs tagged `production_candidate`
3. Check the metrics
4. Download the model
5. Done!

No digging through Slack messages or email chains!

---

### 4. Reproducibility

Every experiment includes:
- **Exact parameters** used
- **Exact metrics** achieved
- **Trained model** (downloadable)
- **Timestamp** (when it was created)
- **Environment** (Python packages used)

**Anyone can reproduce your results!**

---

## Interactive Features to Try

### 1. Chart View

Click the **"Chart"** tab to see:
- Line charts of metrics over time
- Bar charts comparing runs
- Scatter plots of parameters vs metrics

### 2. Download Models

In any run's Artifacts tab:
- Click on `player_detector_medium/`
- Right-click `model.pkl`
- Select "Download"
- Now you have the trained model!

### 3. Register Models

Click **"Register Model"** button:
- Creates a model in the Model Registry
- Assigns version numbers (v1, v2, v3)
- Tracks which version is in production
- Enables model lifecycle management

### 4. Compare Side-by-Side

Select 2+ runs, click "Compare":
- See parameters side-by-side
- See metrics side-by-side
- Identify best performer instantly

---

## Real-World Use Case: Our Soccer Detection

In **TC-003** (our planned test), we would:

### During Training:
```python
with mlflow.start_run(run_name="yolov8n_soccer_detection"):
    # Log training parameters
    mlflow.log_param("model", "yolov8n")
    mlflow.log_param("confidence_threshold", 0.4)
    mlflow.log_param("training_epochs", 50)
    mlflow.log_param("dataset", "synthetic_soccer_field")

    # Train model
    results = train_yolo_model()

    # Log metrics
    mlflow.log_metric("map50", results.metrics.map50)
    mlflow.log_metric("precision", results.metrics.precision)
    mlflow.log_metric("recall", results.metrics.recall)
    mlflow.log_metric("players_detected", 399)

    # Save model
    mlflow.pytorch.log_model(model, "soccer_player_detector")
```

### In the UI, you'd see:
```
Experiment: soccer-player-detection
â”œâ”€â”€ Run 1: yolov8n_soccer_detection
â”‚   â”œâ”€â”€ Parameters: model=yolov8n, confidence=0.4, epochs=50
â”‚   â”œâ”€â”€ Metrics: map50=0.92, precision=0.87, players_detected=399
â”‚   â””â”€â”€ Artifacts: soccer_player_detector/ (trained model)
â”‚
â”œâ”€â”€ Run 2: yolov8s_higher_conf
â”‚   â”œâ”€â”€ Parameters: model=yolov8s, confidence=0.5, epochs=50
â”‚   â”œâ”€â”€ Metrics: map50=0.94, precision=0.91, players_detected=412
â”‚   â””â”€â”€ Artifacts: soccer_player_detector/ (trained model)
â”‚
â””â”€â”€ Run 3: yolov8m_experimental
    â”œâ”€â”€ Parameters: model=yolov8m, confidence=0.3, epochs=100
    â”œâ”€â”€ Metrics: map50=0.89, precision=0.62, players_detected=445
    â””â”€â”€ Artifacts: soccer_player_detector/ (trained model)
```

**Decision**: Run 2 (yolov8s) has best balance â†’ Register as v1.0 â†’ Deploy!

---

## Summary

**MLflow UI shows you:**

1. âœ… **All experiments** in one place (no lost results!)
2. âœ… **Parameters used** for each run
3. âœ… **Metrics achieved** (accuracy, F1, etc.)
4. âœ… **Trained models** (downloadable!)
5. âœ… **Comparison tools** (charts, tables)
6. âœ… **Search & filter** (find best models fast)
7. âœ… **Tags** (production, experimental, etc.)

**It's like GitHub for ML experiments!**

---

## Next Steps

### Refresh the page!
Open http://localhost:5000 and explore:
1. Click on "soccer-player-detection" experiment
2. Click on each run to see details
3. Compare runs using the Compare button
4. Download a model from the Artifacts tab

### Stop the server when done:
```bash
# Find the MLflow process
ps aux | grep mlflow

# Kill it
kill <PID>
```

---

**Enjoy exploring MLflow!** ðŸŽ‰
